{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name:曹人中\n",
    "\n",
    "Student ID:114033584\n",
    "\n",
    "GitHub ID:sam610510\n",
    "\n",
    "Kaggle name:sam0515\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![ranking_scoring_result.png](./pics/ranking_scoring_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "**Syntax:** `###` creates a tertiary heading (H3).\n",
    "\n",
    "[Content for Preprocessing]\n",
    "\n",
    "**Example Syntax for Content:**\n",
    "*   **Bold text:** `**text**`\n",
    "*   *Italic text*: `*text*`\n",
    "*   Bullet point list:\n",
    "    * Item 1\n",
    "    * Item 2\n",
    "\n",
    "Markdown Syntax to Add Image: `![Description of the Image](./your_local_folder/name_of_the_image.png)`\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/example_md_img.png)\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "[Content for Feature Engineering]\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "[Content for Model Explanation]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "[Content for Experiments]\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "[Content for Insights]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在讀取並解析 JSON...\n",
      "正在進行文字清理...\n",
      "資料處理完成！\n",
      "訓練集: 47890 筆, 測試集: 16281 筆\n",
      "標籤對應: {'joy': 0, 'fear': 1, 'anger': 2, 'surprise': 3, 'sadness': 4, 'disgust': 5}\n",
      "\n",
      "處理後的訓練資料範例：\n",
      "                                        cleaned_text emotion  label_id\n",
      "0  i bet there is an army of married couples who ...     joy         0\n",
      "1                         this could only end badly.    fear         1\n",
      "2  my sister squeezed a lime in her milk when she...     joy         0\n",
      "3                                thank you so much❤️     joy         0\n",
      "4  stinks because ive been in this program for a ...     joy         0\n"
     ]
    }
   ],
   "source": [
    "### Add the code related to the preprocessing steps in cells inside this section\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. 定義文字清理函式 (新增部分)\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    \n",
    "    # 轉小寫\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 移除 URL (網址對情緒判斷通常雜訊較大)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 處理 Hashtag: 把 \"#happy\" 變成 \"happy\"，保留語意\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # 移除 Mention (@username): 避免模型過度擬合特定帳號\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # 移除多餘空白\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 優化後的讀取與前處理主函式\n",
    "def load_and_preprocess_data():\n",
    "    print(\"正在讀取並解析 JSON...\")\n",
    "    # 讀取原始 JSON\n",
    "    with open('dm-lab-2-private-competition/final_posts.json', 'r', encoding='utf-8') as f:\n",
    "        posts_data = json.load(f)\n",
    "    \n",
    "    # 解析巢狀結構\n",
    "    posts_list = [item['root']['_source']['post'] for item in posts_data]\n",
    "    posts_df = pd.DataFrame(posts_list)\n",
    "    \n",
    "    # 統一欄位名稱\n",
    "    posts_df.rename(columns={'post_id': 'id'}, inplace=True)\n",
    "    \n",
    "    # 在這裡就先做文字清理，避免帶著髒資料 merge\n",
    "    print(\"正在進行文字清理...\")\n",
    "    posts_df['cleaned_text'] = posts_df['text'].apply(clean_text)\n",
    "    \n",
    "    # 處理 Hashtags (選用) ---\n",
    "    # 有些模型若覺得 text 不夠，可以把 hashtags 串接在後面加強語氣\n",
    "    posts_df['combined_text'] = posts_df['cleaned_text'] + \" \" + posts_df['hashtags'].apply(lambda x: \" \".join(x) if isinstance(x, list) else \"\")\n",
    "\n",
    "    # 讀取輔助 CSV\n",
    "    emotion_df = pd.read_csv('dm-lab-2-private-competition/emotion.csv')\n",
    "    data_id_df = pd.read_csv('dm-lab-2-private-competition/data_identification.csv')\n",
    "\n",
    "    # 資料合併\n",
    "    merged_df = pd.merge(posts_df, data_id_df, on='id', how='left')\n",
    "    \n",
    "    # 分割訓練集與測試集\n",
    "    train_df = merged_df[merged_df['split'] == 'train'].copy()\n",
    "    test_df = merged_df[merged_df['split'] == 'test'].copy()\n",
    "\n",
    "    # 合併情緒標籤\n",
    "    train_df = pd.merge(train_df, emotion_df, on='id', how='left')\n",
    "    \n",
    "    # 標籤數值化 (Label Encoding)\n",
    "    # 將 'joy', 'anger' 轉為 0, 1, 2...\n",
    "    label_mapping = {label: idx for idx, label in enumerate(train_df['emotion'].unique())}\n",
    "    # 反向對照表 (之後預測轉回文字用)\n",
    "    id2label = {idx: label for label, idx in label_mapping.items()}\n",
    "    \n",
    "    train_df['label_id'] = train_df['emotion'].map(label_mapping)\n",
    "    \n",
    "    print(f\"資料處理完成！\")\n",
    "    print(f\"訓練集: {len(train_df)} 筆, 測試集: {len(test_df)} 筆\")\n",
    "    print(f\"標籤對應: {label_mapping}\")\n",
    "    \n",
    "    return train_df, test_df, label_mapping, id2label\n",
    "\n",
    "# 執行\n",
    "train_df, test_df, label_mapping, id2label = load_and_preprocess_data()\n",
    "\n",
    "# 檢查一下結果\n",
    "print(\"\\n處理後的訓練資料範例：\")\n",
    "print(train_df[['cleaned_text', 'emotion', 'label_id']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the feature engineering steps in cells inside this section\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 初始化 TF-IDF 向量轉換器\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,      # 限制特徵數量，選取最重要的前 5000 個字\n",
    "    stop_words=None,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=3,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True    # 去除英文停用詞\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在訓練模型 (啟用 Class Weight 平衡)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在預測測試集...\n",
      "檔案已儲存: dm-lab-2-private-competition/submission.csv\n"
     ]
    }
   ],
   "source": [
    "### Add the code related to the model implementation steps in cells inside this section\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def train_and_predict(train_df, test_df):\n",
    "    # 1. [修正] 使用前處理過的 'cleaned_text' (如果前一步沒跑，請改回 'text')\n",
    "    # 這是提升準度的基礎\n",
    "    X_train = train_df['cleaned_text']\n",
    "    y_train = train_df['emotion']\n",
    "    X_test = test_df['cleaned_text']\n",
    "\n",
    "    # 建立 Pipeline (整合 Feature Engineering + 強化的模型)\n",
    "    model_pipeline = Pipeline([\n",
    "        # Feature Engineering) \n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            max_features=10000,      # 增加特徵量，讓模型看更多單字\n",
    "            ngram_range=(1, 2),      # 開啟 Bigrams，捕捉 \"not good\" 這種詞組\n",
    "            stop_words=None,         # 保留 \"not\", \"no\" 等否定詞\n",
    "            min_df=3,\n",
    "            sublinear_tf=True        # 平滑化頻率，避免某些字出現太多次影響判斷\n",
    "        )),\n",
    "        \n",
    "        # Model\n",
    "        ('clf', LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            multi_class='ovr',       # One-vs-Rest\n",
    "            class_weight='balanced',\n",
    "            solver='liblinear',      \n",
    "            C=1.0                    # 正則化係數\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # 訓練模型\n",
    "    print(\"正在訓練模型 (啟用 Class Weight 平衡)...\")\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # 進行預測\n",
    "    print(\"正在預測測試集...\")\n",
    "    predictions = model_pipeline.predict(X_test)\n",
    "\n",
    "    # 格式化輸出\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'emotion': predictions\n",
    "    })\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# 執行模型訓練與預測\n",
    "submission = train_and_predict(train_df, test_df)\n",
    "try:\n",
    "    sample_sub_df = pd.read_csv('dm-lab-2-private-competition/samplesubmission.csv')\n",
    "    submission = submission.set_index('id').reindex(sample_sub_df['id']).reset_index()\n",
    "except FileNotFoundError:\n",
    "    print(\"找不到 samplesubmission.csv，將跳過排序步驟直接存檔。\")\n",
    "\n",
    "# 存檔\n",
    "output_file = 'dm-lab-2-private-competition/submission.csv'\n",
    "submission.to_csv(output_file, index=False)\n",
    "print(f\"檔案已儲存: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在訓練 SVM 模型...\n",
      "正在預測...\n",
      "SVM 結果已儲存: submission_svm.csv\n"
     ]
    }
   ],
   "source": [
    "# LinearSVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 建立 Pipeline\n",
    "svm_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=10000, \n",
    "        ngram_range=(1, 2), \n",
    "        stop_words=None,\n",
    "        sublinear_tf=True\n",
    "    )),\n",
    "    ('clf', LinearSVC(\n",
    "        class_weight='balanced', # 處理資料不平衡\n",
    "        dual=False,            \n",
    "        max_iter=3000,\n",
    "        C=0.5               \n",
    "    ))\n",
    "])\n",
    "\n",
    "# 2. 訓練\n",
    "print(\"正在訓練 SVM 模型...\")\n",
    "svm_pipeline.fit(train_df['cleaned_text'], train_df['label_id'])\n",
    "\n",
    "# 3. 預測\n",
    "print(\"正在預測...\")\n",
    "preds_idx = svm_pipeline.predict(test_df['cleaned_text'])\n",
    "\n",
    "# 4. 轉回文字標籤並存檔\n",
    "preds_label = [id2label[idx] for idx in preds_idx]\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'emotion': preds_label})\n",
    "\n",
    "# 排序 (選用)\n",
    "try:\n",
    "    sample_sub = pd.read_csv('dm-lab-2-private-competition/samplesubmission.csv')\n",
    "    submission = submission.set_index('id').reindex(sample_sub['id']).reset_index()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "submission.to_csv('dm-lab-2-private-competition/submission_svm.csv', index=False)\n",
    "print(\"SVM 結果已儲存: submission_svm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在訓練 Gradient Boosting 模型 \n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3412            4.69m\n",
      "         2           1.3191            4.64m\n",
      "         3           1.3002            4.61m\n",
      "         4           1.2856            4.55m\n",
      "         5           1.2731            4.48m\n",
      "         6           1.2623            4.42m\n",
      "         7           1.2526            4.36m\n",
      "         8           1.2421            4.30m\n",
      "         9           1.2344            4.25m\n",
      "        10           1.2264            4.20m\n",
      "        20           1.1695            3.72m\n",
      "        30           1.1296            3.24m\n",
      "        40           1.0986            2.77m\n",
      "        50           1.0722            2.31m\n",
      "        60           1.0491            1.84m\n",
      "        70           1.0286            1.38m\n",
      "        80           1.0108           55.19s\n",
      "        90           0.9944           27.56s\n",
      "       100           0.9805            0.00s\n",
      "正在預測...\n",
      "Gradient Boosting 結果已儲存: dm-lab-2-private-competition/submission_gb.csv\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# 使用 sklearn 內建的 GradientBoostingClassifier\n",
    "gb_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=5000,       \n",
    "        ngram_range=(1, 2), \n",
    "        stop_words=None\n",
    "    )),\n",
    "    ('clf', GradientBoostingClassifier(\n",
    "        n_estimators=100,        # 樹的數量\n",
    "        learning_rate=0.1,       # 學習率\n",
    "        max_depth=5,             # 樹的深度\n",
    "        random_state=42,\n",
    "        verbose=1               \n",
    "    ))\n",
    "])\n",
    "\n",
    "# 訓練\n",
    "print(\"正在訓練 Gradient Boosting 模型 \")\n",
    "gb_pipeline.fit(train_df['cleaned_text'], train_df['label_id'])\n",
    "\n",
    "# 預測\n",
    "print(\"正在預測...\")\n",
    "preds_idx = gb_pipeline.predict(test_df['cleaned_text'])\n",
    "\n",
    "# 轉回文字標籤\n",
    "preds_label = [id2label[idx] for idx in preds_idx]\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'emotion': preds_label})\n",
    "\n",
    "# 排序\n",
    "try:\n",
    "    sample_sub = pd.read_csv('dm-lab-2-private-competition/samplesubmission.csv')\n",
    "    submission = submission.set_index('id').reindex(sample_sub['id']).reset_index()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 存檔 \n",
    "output_file = 'dm-lab-2-private-competition/submission_gb.csv'\n",
    "submission.to_csv(output_file, index=False)\n",
    "print(f\"Gradient Boosting 結果已儲存: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "正在使用的裝置: CUDA\n",
      "正在讀取並清理資料...\n",
      "資料載入完成！訓練集: 47890 筆\n",
      "正在 Tokenization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec57c368b5d4a7ab768538bcef2942f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/43101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b42acfd7f0b45e99a7a5cf72bef6eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4789 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01659047f8444c40bc7200fb81583229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16316\\3126571770.py:113: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始訓練\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5388' max='5388' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5388/5388 08:23, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.884600</td>\n",
       "      <td>0.868260</td>\n",
       "      <td>0.685529</td>\n",
       "      <td>0.496850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.761300</td>\n",
       "      <td>0.869965</td>\n",
       "      <td>0.686156</td>\n",
       "      <td>0.513306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在預測...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成！檔案已儲存: dm-lab-2-private-competition/submission_bert_local.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# 準備工作\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"正在使用的裝置: {device.upper()}\")\n",
    "\n",
    "# 設定隨機種子 (讓結果可重現)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 資料讀取與前處理\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def load_data_local():\n",
    "    print(\"正在讀取並清理資料...\")\n",
    "    try:\n",
    "        with open('dm-lab-2-private-competition/final_posts.json', 'r', encoding='utf-8') as f:\n",
    "            posts_data = json.load(f)\n",
    "        posts_df = pd.DataFrame([item['root']['_source']['post'] for item in posts_data])\n",
    "        posts_df.rename(columns={'post_id': 'id'}, inplace=True)\n",
    "        \n",
    "        posts_df['cleaned_text'] = posts_df['text'].apply(clean_text)\n",
    "        \n",
    "        emotion_df = pd.read_csv('dm-lab-2-private-competition/emotion.csv')\n",
    "        data_id_df = pd.read_csv('dm-lab-2-private-competition/data_identification.csv')\n",
    "        \n",
    "        merged_df = pd.merge(posts_df, data_id_df, on='id', how='left')\n",
    "        train_full = merged_df[merged_df['split'] == 'train'].copy()\n",
    "        test_df = merged_df[merged_df['split'] == 'test'].copy()\n",
    "        \n",
    "        train_full = pd.merge(train_full, emotion_df, on='id', how='left')\n",
    "        \n",
    "        # Label Encoding\n",
    "        label_list = train_full['emotion'].unique()\n",
    "        label2id = {label: i for i, label in enumerate(label_list)}\n",
    "        id2label = {i: label for label, i in label2id.items()}\n",
    "        \n",
    "        train_full['label'] = train_full['emotion'].map(label2id)\n",
    "        \n",
    "        return train_full, test_df, id2label, label2id\n",
    "    except FileNotFoundError:\n",
    "        print(\"找不到檔案！請檢查路徑 (例如是否在 dm-lab-2-private-competition 資料夾下？)\")\n",
    "        raise\n",
    "\n",
    "# 執行載入\n",
    "train_full, test_df, id2label, label2id = load_data_local()\n",
    "print(f\"資料載入完成！訓練集: {len(train_full)} 筆\")\n",
    "\n",
    "# 切分驗證集\n",
    "train_df, val_df = train_test_split(train_full, test_size=0.1, stratify=train_full['label'], random_state=42)\n",
    "\n",
    "# 轉成 Dataset\n",
    "hf_train = Dataset.from_pandas(train_df[['cleaned_text', 'label']].rename(columns={'cleaned_text': 'text'}))\n",
    "hf_val = Dataset.from_pandas(val_df[['cleaned_text', 'label']].rename(columns={'cleaned_text': 'text'}))\n",
    "hf_test = Dataset.from_pandas(test_df[['cleaned_text']].rename(columns={'cleaned_text': 'text'}))\n",
    "\n",
    "# BERT 模型設定\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=128)\n",
    "\n",
    "print(\"正在 Tokenization...\")\n",
    "tokenized_train = hf_train.map(preprocess_function, batched=True)\n",
    "tokenized_val = hf_val.map(preprocess_function, batched=True)\n",
    "tokenized_test = hf_test.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=len(label2id))\n",
    "model.to(device)\n",
    "\n",
    "# 訓練參數 (針對本機優化)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_local_results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,             \n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    eval_strategy=\"epoch\",          \n",
    "    \n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    dataloader_num_workers=0,       \n",
    "    report_to=\"none\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "\n",
    "# 執行\n",
    "print(\"開始訓練\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"正在預測...\")\n",
    "test_predictions = trainer.predict(tokenized_test)\n",
    "preds_idx = np.argmax(test_predictions.predictions, axis=1)\n",
    "preds_label = [id2label[idx] for idx in preds_idx]\n",
    "\n",
    "# 存檔\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'emotion': preds_label\n",
    "})\n",
    "\n",
    "# 排序\n",
    "try:\n",
    "    sample_sub = pd.read_csv('dm-lab-2-private-competition/samplesubmission.csv')\n",
    "    submission = submission.set_index('id').reindex(sample_sub['id']).reset_index()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "output_file = 'dm-lab-2-private-competition/submission_bert_local.csv'\n",
    "submission.to_csv(output_file, index=False)\n",
    "print(f\"完成！檔案已儲存: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在進行 RoBERTa Tokenization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172b181e9a15410f888e2b05132f158a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38790 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99ceedb6ff148eb873311d0e9df3c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7609f36502a241698629ef60d9af3902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "類別權重已計算: [0.33540856 3.97357099 0.74636343 1.27063679 2.03301887 6.74843424]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16316\\1841134756.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始訓練 RoBERTa\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7275' max='7275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7275/7275 24:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.288600</td>\n",
       "      <td>1.265409</td>\n",
       "      <td>0.599861</td>\n",
       "      <td>0.491758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.104800</td>\n",
       "      <td>1.232597</td>\n",
       "      <td>0.639991</td>\n",
       "      <td>0.501269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.978100</td>\n",
       "      <td>1.263112</td>\n",
       "      <td>0.638367</td>\n",
       "      <td>0.517139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在預測...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa 結果已儲存: dm-lab-2-private-competition/submission_roberta_weighted.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn as nn\n",
    "\n",
    "# RoBERTa\n",
    "# 原始 BERT: \"distilbert-base-uncased\"\n",
    "# 升級: \"roberta-base\" \n",
    "model_checkpoint = \"roberta-base\" \n",
    "\n",
    "# 切分驗證集\n",
    "train_split, val_split = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['label'])\n",
    "\n",
    "# 轉成 Dataset\n",
    "hf_train = Dataset.from_pandas(train_split[['cleaned_text', 'label']].rename(columns={'cleaned_text': 'text'}))\n",
    "hf_val = Dataset.from_pandas(val_split[['cleaned_text', 'label']].rename(columns={'cleaned_text': 'text'}))\n",
    "hf_test = Dataset.from_pandas(test_df[['cleaned_text']].rename(columns={'cleaned_text': 'text'}))\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # 增加 max_length 到 256，捕捉更長的文章細節\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=256) \n",
    "\n",
    "print(\"正在進行 RoBERTa Tokenization...\")\n",
    "tokenized_train = hf_train.map(preprocess_function, batched=True)\n",
    "tokenized_val = hf_val.map(preprocess_function, batched=True)\n",
    "tokenized_test = hf_test.map(preprocess_function, batched=True)\n",
    "\n",
    "# 計算類別權重 (解決不平衡)\n",
    "# 計算每個類別的權重：數量越少的類別，權重越大\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced', \n",
    "    classes=np.unique(train_split['label']), \n",
    "    y=train_split['label']\n",
    ")\n",
    "# 轉成 Tensor 並搬到 GPU (如果有)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(f\"類別權重已計算: {class_weights}\")\n",
    "\n",
    "# 自定義 Trainer 以支援加權損失\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # 前向傳播\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # 定義加權的 CrossEntropyLoss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# 設定訓練參數與模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=len(label2id))\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta_results\",\n",
    "    learning_rate=2e-5,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,              \n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",           \n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\", \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 評估函式\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
    "\n",
    "# 初始化我們自定義的 Trainer\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "# 開始訓練\n",
    "print(\"開始訓練 RoBERTa\")\n",
    "trainer.train()\n",
    "\n",
    "# 預測與存檔\n",
    "print(\"正在預測...\")\n",
    "test_predictions = trainer.predict(tokenized_test)\n",
    "preds_idx = np.argmax(test_predictions.predictions, axis=1)\n",
    "preds_label = [id2label[idx] for idx in preds_idx]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'emotion': preds_label\n",
    "})\n",
    "\n",
    "try:\n",
    "    sample_sub = pd.read_csv('dm-lab-2-private-competition/samplesubmission.csv')\n",
    "    submission = submission.set_index('id').reindex(sample_sub['id']).reset_index()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "output_file = 'dm-lab-2-private-competition/submission_roberta_weighted.csv'\n",
    "# 確保目錄存在\n",
    "import os\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "submission.to_csv(output_file, index=False)\n",
    "print(f\"RoBERTa 結果已儲存: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先Logistic Regressio與 Linear SVM 都屬於線性分類器，依賴 TF-IDF 這類高維稀疏向量進行判斷；Logistic Regressio以線性邊界快速建構基準線，但對否定語句與語意互動毫無感知，而Linear SVM透過最大化分類間隔在泛化能力上有更好的表現，通常能提供更穩定、較高的準確率。然而XGBoost 則跳脫線性框架，利用樹模型捕捉非線性關係，特別適合搭配人工特徵來萃取語用訊號，但若只依賴 TF-IDF，其表現會與 SVM 相近並且訓練成本更高。深度學習階段的 BERT/DistilBERT 透過 Transformer 直接建構上下文語意表示，使模型具備理解否定、一字多義、語境與反諷的能力，使表現大幅超越所有傳統模型。最終的 RoBERTa + Weighted Loss 在 BERT 的基礎上，以更大量多樣的語料強化語言感知，同時透過加權損失處理情緒資料中嚴重的類別不平衡，使模型不能再依賴預測多數類別取分，而是更精準捕捉少數情緒的語言特徵。最後雖然RoBERTa + Weighted Loss 理論上比 BERT 更強，但在這份資料上反而表現較差，可能是因為 RoBERTa 的預訓練資料較偏向口語與網路文本，與這次資料集的語言風格不夠貼近；而加權損失在資料量有限時也可能造成少數類別的過度補償，降低整體穩定性。因此 BERT 在此任務中反而取得較佳成效。整體而言，這五種方法從「線性關係」到「非線性關係」，再到「深層語意理解」，反映了 NLP 在面對語言複雜性時逐漸遞進的建模能力差異。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab2-Exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
